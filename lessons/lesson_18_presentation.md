---
marp: true
paginate: false

style: |
  table {
    font-size: 0.7em;
  }
  section {
    font-size: 3em;
  }
  section.centered {
    display: flex;
    flex-direction: column;
    justify-content: center;
    text-align: center;
  }
  .two-columns {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 40px;
    align-items: center;
  }
---
<!-- _class: centered -->
# Random Forest
## Ансамблевые методы

---
# Что мы уже знаем?

**Decision Tree** - дерево решений с if-else правилами

**Проблема:**
- Сильный overfitting
- Train accuracy = 98%, Test accuracy = 75%
- Слишком чувствительны к данным
- Малые изменения в данных -> совсем другое дерево

**Решение:** Random Forest!

---
# Мудрость толпы

**Вопрос:** Сколько шариков в банке?

**Один эксперт:** может ошибиться на 50%

**100 человек:** средняя оценка близка к правде

**Принцип:** усреднение независимых мнений снижает ошибку

---
**Random Forest** = ансамбль (лес) из множества деревьев решений
**Идея:**
1. Обучить **много** разных деревьев (100-1000)
2. Каждое дерево делает предсказание
3. **Голосование:** финальное предсказание = мнение большинства

**Классификация:** выбирается класс, за который проголосовало больше деревьев
**Регрессия:** усредняются предсказания всех деревьев

---
**Проблема:** одинаковые деревья
**Решение 1: Bootstrap (Bagging)**
- Каждое дерево обучается на случайной выборке с возвращением
- Размер выборки = размер исходных данных
- ~63% данных попадает в каждую выборку
**Решение 2: Random Subspace**
- При каждом разделении используется случайное подмножество признаков

---
# Bootstrap Aggregating (Bagging)

**Bootstrap:**
```
Исходные данные: [1, 2, 3, 4, 5]

Дерево 1: [1, 1, 3, 4, 5]  ← с возвращением
Дерево 2: [2, 2, 2, 3, 4]
Дерево 3: [1, 3, 4, 5, 5]
```

Каждое дерево видит немного разные данные -> разные деревья

**Aggregating:** комбинирование предсказаний

---
1. Выбрать `n_estimators` (например, 100)
2. Для каждого дерева:
   - Создать bootstrap выборку
   - При каждом split:
     * Выбрать случайное подмножество признаков
     * Найти лучшее разделение среди них
   - Обучить дерево (обычно без ограничения глубины)
3. Каждое дерево делает предсказание:
   - **Классификация:** мажоритарное голосование
   - **Регрессия:** среднее значение

---
# Пример: Предсказание болезни сердца

**Задача:** предсказать наличие сердечного заболевания

**100 деревьев делают предсказания:**
- 73 дерева: болезнь есть (1)
- 27 деревьев: болезни нет (0)

**Финальное предсказание:** 1 (больной)
**Уверенность:** 73% вероятность болезни

---
# Почему Random Forest лучше

**Variance Reduction (снижение дисперсии):**
- Одно дерево может сильно ошибаться
- Среднее из многих деревьев стабильнее

**Защита от overfitting:**
- Каждое дерево видит разные данные
- Не все деревья запомнят один и тот же шум

**Bias остается:** Random Forest не хуже отдельного дерева

---
# Out-of-Bag (OOB) Score

**Проблема:** нужно оценить качество без test set

**Решение:** Out-of-Bag примеры

При bootstrap ~37% данных **не попадают** в выборку для дерева
-> эти данные можно использовать как validation set

**OOB Score** - качество предсказаний на OOB примерах
- Не нужно делить данные на train/test

---
# Гиперпараметры Random Forest

| Параметр | Описание | Типичные значения |
|----------|----------|-------------------|
| `n_estimators` | Количество деревьев | 100-1000 |
| `max_depth` | Максимальная глубина дерева | None или 10-30 |
| `min_samples_split` | Минимум для разделения | 2-20 |
| `min_samples_leaf` | Минимум в листе | 1-10 |
| `max_features` | Признаков для split | 'sqrt', 'log2', или число |
| `bootstrap` | Использовать ли bootstrap | True |
| `oob_score` | Вычислять OOB score | True/False |

---
# n_estimators: Больше = лучше?

**Правило:** больше деревьев -> лучше качество -> медленнее обучение

**На практике:**
- 100 деревьев - baseline
- 500-1000 - для продакшна
- Больше 1000 - редко дает улучшение

Random Forest не переобучается от большого `n_estimators`

---
# max_features: Сколько признаков?

**Для классификации:** `max_features='sqrt'` (по умолчанию)
- Если 16 признаков -> используется 4 случайных для каждого split

**Для регрессии:** `max_features=1.0` (все признаки)

**Зачем ограничивать?**
- Больше разнообразия между деревьями
- Быстрее обучение

---
# Feature Importance

**Как в дереве:** важность = среднее снижение Gini/MSE

**В Random Forest:** усредняется по всем деревьям

```python
rf.feature_importances_
```
**Преимущество:** более стабильная оценка, чем в одном дереве


---
# Преимущества Random Forest
 - **Высокая точность** 
 - **Устойчивость к overfitting** - лучше чем одно дерево
 - **Не требует масштабирования** - работает с исходными данными
 - **Feature importance** - понятная интерпретация
 - **Робастность к выбросам** - голосование смягчает эффект
 - **Параллелизация** - деревья обучаются независимо

---
# Недостатки Random Forest

 **Размер модели** - сотни деревьев занимают много памяти
 **Скорость предсказания** - нужно опросить все деревья
 **Черный ящик** - сложнее интерпретировать чем одно дерево
 **Медленнее чем линейные модели** на очень больших данных

---
# Когда использовать Random Forest?

**Хорошо подходит:**
- Нелинейные зависимости
- Смешанные типы признаков
- Выбросы в данных
- Нужна хорошая точность "из коробки"
- Средние размеры датасетов (1K-1M строк)
---
**Не подходит:**
- Очень большие данные (>10M строк) -> Gradient Boosting
- Нужна максимальная интерпретируемость -> Logistic Regression
- Реалтайм предсказания с минимальной задержкой
- Линейные зависимости -> линейные модели эффективнее

---
# Связь с другими методами

**Random Forest** - представитель семейства **Bagging**

**Bagging:**
- Random Forest (деревья)
- Bagged SVM, Bagged Logistic Regression

**Boosting:** 
- Gradient Boosting
- XGBoost, LightGBM, CatBoost

---
# Основные идеи

1. **Ансамбль** лучше одной модели
2. **Bootstrap** создает разнообразие в данных
3. **Random features** создает разнообразие в признаках
4. **Голосование** снижает variance (дисперсию)
5. **OOB score** - бесплатная валидация
6. **Feature importance** - понимание данных
7. **Параллелизация** - быстрое обучение