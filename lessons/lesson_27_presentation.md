---
marp: true
paginate: false

style: |
  table {
    font-size: 0.7em;
  }
  section {
    font-size: 3em;
  }
  section.centered {
    display: flex;
    flex-direction: column;
    justify-content: center;
    text-align: center;
  }
  .two-columns {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 40px;
    align-items: center;
  }
---
<!-- _class: centered -->
# Naive Bayes Classifier
## Вероятностный подход к классификации

---
Вычислить вероятность класса при наличии признаков

$$P(y|X) = \frac{P(X|y) \cdot P(y)}{P(X)}$$

Где:
- $P(y|X)$ - вероятность класса $y$ при признаках $X$ (posterior)
- $P(X|y)$ - вероятность признаков $X$ при классе $y$ (likelihood)
- $P(y)$ - априорная вероятность класса $y$ (prior)
- $P(X)$ - вероятность признаков $X$ (evidence)

---
# Наивное предположение

**"Naive" (наивный)** - все признаки независимы друг от друга

$$P(X|y) = P(x_1|y) \cdot P(x_2|y) \cdot ... \cdot P(x_n|y)$$

**Пример:**
При классификации email как спам:
- Слово "бесплатно" не зависит от слова "выигрыш"

---

**Классификация:**
1. Вычисляем $P(y_k|X)$ для каждого класса $y_k$
2. Выбираем класс с максимальной вероятностью

$$\hat{y} = \arg\max_{y_k} P(y_k) \prod_{i=1}^{n} P(x_i|y_k)$$

**На практике:** используем логарифмы для избежания underflow

$$\hat{y} = \arg\max_{y_k} \left[\log P(y_k) + \sum_{i=1}^{n} \log P(x_i|y_k)\right]$$

---

**1. Gaussian Naive Bayes**
- Для непрерывных признаков
- Предполагает нормальное распределение
- Применение: числовые данные

**2. Multinomial Naive Bayes**
- Для дискретных признаков (счетчиков)
- Применение: текстовые данные, количество слов

**3. Bernoulli Naive Bayes**
- Для бинарных признаков (0/1)

---
# Gaussian Naive Bayes


$$P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2}\right)$$

Где:
- $\mu_y$ - среднее значение признака для класса $y$
- $\sigma_y^2$ - дисперсия признака для класса $y$

**Простыми словами:** вычисляем среднее и стандартное отклонение для каждого признака в каждом классе

---

<div class="two-columns">

<div>

**Данные:**
- Длина чашелистика
- Ширина чашелистика
- Длина лепестка
- Ширина лепестка

**Классы:**
- Setosa
- Versicolor
- Virginica

</div>

<div>

**Процесс:**
1. Вычислить $\mu$ и $\sigma$ для каждого признака
2. Для нового объекта вычислить $P(y_k|X)$
3. Выбрать класс с max вероятностью

</div>

</div>

---
# Multinomial Naive Bayes


$$P(x_i|y) = \frac{N_{yi} + \alpha}{N_y + \alpha n}$$

Где:
- $N_{yi}$ - количество раз, когда слово $i$ встречается в классе $y$
- $N_y$ - общее количество слов в классе $y$
- $\alpha$ - параметр сглаживания Лапласа (обычно 1)
- $n$ - размер словаря

---
# Пример: классификация текста

**Задача:** определить, является ли email спамом

```
Email 1 (spam): "Бесплатный выигрыш! Бесплатно!"
Email 2 (ham):  "Встреча завтра в офисе"
```

**Признаки:** количество каждого слова

| Слово | Spam | Ham |
|-------|------|-----|
| бесплатный | 2 | 0 |
| выигрыш | 1 | 0 |
| встреча | 0 | 1 |
| офис | 0 | 1 |

---
# Bernoulli Naive Bayes

**Применение:** бинарные признаки (есть/нет)

$$P(x_i|y) = P(i|y) \cdot x_i + (1 - P(i|y)) \cdot (1 - x_i)$$

Где:
- $x_i \in \{0, 1\}$
- $P(i|y)$ - вероятность того, что признак $i$ присутствует в классе $y$

**Пример:** документ содержит определенные ключевые слова (да/нет)

---
# Преимущества Naive Bayes

**Быстрый:** обучение и предсказание
**Простой:** легко понять и интерпретировать
**Эффективен для маленьких датасетов**
**Хорош для многомерных данных:** не страдает от "проклятия размерности"
**Работает хорошо для текста:** spam detection, sentiment analysis
**Вероятностные предсказания:** можно получить probability scores

---
# Недостатки Naive Bayes

**Наивное предположение:** признаки редко независимы
**Zero Frequency Problem:** если признак не встречался в обучающей выборке
   - Решение: сглаживание Лапласа
**Плохо работает с коррелирующими признаками**
**Не всегда дает лучшую точность** по сравнению с другими методами

---
# Когда использовать Naive Bayes?

**Хорошо подходит для:**
- Классификация текста (spam, sentiment)
- Многоклассовая классификация
- Рекомендательные системы
- Медицинская диагностика (при условии независимости симптомов)
- Real-time predictions (быстрый)

---

**Не подходит для:**
- Сильно коррелирующие признаки
- Когда нужна высокая точность (лучше ensemble методы)
- Регрессия (это классификатор)

---
# Сглаживание Лапласа

**Проблема:** что если слово никогда не встречалось в классе?
$P(x_i|y) = 0$

**Без сглаживания:**
$$P(x_i|y) = \frac{count(x_i, y)}{count(y)}$$

**Со сглаживанием:**
$$P(x_i|y) = \frac{count(x_i, y) + \alpha}{count(y) + \alpha n}$$

---
# Naive Bayes в scikit-learn

```python
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

# Gaussian (для числовых признаков)
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# Multinomial (для текста, счетчиков)
mnb = MultinomialNB(alpha=1.0)  # alpha - сглаживание
mnb.fit(X_train, y_train)

# Bernoulli (для бинарных признаков)
bnb = BernoulliNB(alpha=1.0)
bnb.fit(X_train, y_train)
```

---
# Метрики качества

**Для классификации:**
- **Accuracy** - общая точность
- **Precision** - точность для каждого класса
- **Recall** - полнота для каждого класса
- **F1-score** - гармоническое среднее precision и recall
- **Confusion Matrix** - матрица ошибок


---

**Получить вероятности:**

```python
# Вероятности для каждого класса
probabilities = model.predict_proba(X_test)

# Логарифм вероятностей (более стабильно численно)
log_probs = model.predict_log_proba(X_test)
```

**Применение:**
- Ранжирование предсказаний
- Установка порога принятия решения
- Оценка уверенности модели

---
# Практические советы

1. **Выбор типа Naive Bayes:**
   - Числовые данные -> Gaussian
   - Текст, счетчики -> Multinomial
   - Бинарные признаки -> Bernoulli

2. **Используйте сглаживание** (alpha > 0)

3. **Масштабирование не требуется** для Gaussian NB

4. Качество признаков влияет на результат