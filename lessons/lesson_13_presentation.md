---
marp: true
paginate: false

style: |
  table {
    font-size: 0.7em;
  }
  section {
    font-size: 3em;
  }
  section.centered {
    display: flex;
    flex-direction: column;
    justify-content: center;
    text-align: center;
  }
  .two-columns {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 40px;
    align-items: center;
  }
---
<!-- _class: centered -->
# Как работает регрессия изнутри
## Gradient Descent

---
# Linear regression в scikit

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X, y)
predictions = model.predict(X)
```

Что происходит внутри `.fit()`?

---
# Линейная регрессия

### Формула:
$$y = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n$$

- $y$ - предсказываемое значение
- $w_0$ - **intercept** (свободный член)
- $w_1, w_2, ..., w_n$ - **коэффициенты** (веса)
- $x_1, x_2, ..., x_n$ - **признаки**

**Задача:** найти оптимальные $w_0, w_1, ..., w_n$

---
# Loss Function (Функция потерь)

### Mean Squared Error (MSE)

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

Где:
- $y_i$ - реальное значение
- $\hat{y}_i$ - предсказанное значение
- $n$ - количество примеров

---
# Как найти оптимальные веса?

1. **Closed-Form Solution** (аналитическое решение)
   - Решить систему уравнений математически
   - Быстро, но требует много памяти
   - Используется в sklearn по умолчанию

2. **Gradient Descent** (градиентный спуск)
   - Итеративно улучшать веса
   - Медленнее, но работает для больших данных
   
---
### Gradient Descent: Интуиция
"Вы в тумане на горе" и хотите спуститься вниз:
1. Определяете направление наибольшего спуска
2. Делаете шаг в этом направлении
3. Повторяете, пока не достигнете дна

**Градиентный спуск** работает так же:
- "Гора" = функция потерь (MSE)
- "Направление" = градиент (производная)
- "Шаг" = learning rate

---
# Градиент (Gradient)

**Градиент** - это вектор частных производных, показывающий направление наибольшего роста функции.


$$\frac{\partial MSE}{\partial w_0} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)$$

$$\frac{\partial MSE}{\partial w_1} = -\frac{2}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i) \cdot x_i$$

**Минус градиент** = направление уменьшения ошибки

---
# Формулы обновления весов

### Gradient Descent Update Rule:

$$w_0^{new} = w_0^{old} - \alpha \cdot \frac{\partial MSE}{\partial w_0}$$

$$w_1^{new} = w_1^{old} - \alpha \cdot \frac{\partial MSE}{\partial w_1}$$

Где:
- $\alpha$ (**learning rate**) - размер шага
- $\frac{\partial MSE}{\partial w}$ - градиент

---
# Learning Rate (Скорость обучения)

**Learning Rate** ($\alpha$) - один из самых важных гиперпараметров!

| Значение $\alpha$ | Поведение |
|-------------------|-----------|
| Слишком маленькое (0.001) | Медленное обучение, много итераций |
| Оптимальное (0.01-0.1) | Быстрая сходимость |
| Слишком большое (1.0+) | Модель не сходится, "прыгает" |


---
```python
# 1. Инициализация весов
w0 = 0
w1 = 0

# 2. Повторять N итераций:
for i in range(n_iterations):
    # 2.1 Предсказания
    y_pred = w0 + w1 * X
    
    # 2.2 Вычисление MSE
    mse = mean((y - y_pred)²)
    
    # 2.3 Вычисление градиентов
    dw0 = -2/n * sum(y - y_pred)
    dw1 = -2/n * sum((y - y_pred) * X)
    
    # 2.4 Обновление весов
    w0 = w0 - learning_rate * dw0
    w1 = w1 - learning_rate * dw1
```

---

<div class="two-columns">
<div>

### Итерация 1:
- MSE = 150.5
- $w_0$ = 0.0
- $w_1$ = 0.0

### Итерация 100:
- MSE = 45.2
- $w_0$ = 4.3
- $w_1$ = 2.1

</div>
<div>

### Итерация 500:
- MSE = 12.8
- $w_0$ = 6.7
- $w_1$ = 2.9

### Итерация 1000:
- MSE = 12.1
- $w_0$ = 6.95
- $w_1$ = 2.98

</div>
</div>


---

**Сходимость(Convergence)** - когда веса перестают изменяться, и MSE достигает минимума.

### Признаки сходимости:
- MSE почти не меняется между итерациями
- Веса стабилизировались, градиент близок к нулю

### Критерии остановки:
- Достигнуто максимальное количество итераций
- Изменение MSE < пороговое значение (например, 0.0001)

---
# Проблемы Gradient Descent

### 1. Локальные минимумы
- Для линейной регрессии не проблема (выпуклая функция)
- Для нелинейных моделей может застрять

### 2. Масштаб признаков
- Разные масштабы -> разные скорости обучения
- **Решение:** нормализация/стандартизация данных


---
# Важность масштабирования

### Без масштабирования:
```
X1: [1, 2, 3]        -> шаги по w1 маленькие
X2: [1000, 2000, 3000] -> шаги по w2 огромные
```

### С масштабированием (StandardScaler):
```
X1_scaled: [-1, 0, 1]  -> одинаковые
X2_scaled: [-1, 0, 1]  -> масштабы
```


---
# sklearn и наша реализация

| Аспект | sklearn | Наша реализация |
|--------|---------|-----------------|
| Метод | Closed-form solution | Gradient Descent |
| Скорость | Очень быстро | Медленнее |
| Память | Больше | Меньше |
| Большие данные | Может быть проблема | Хорошо работает |
| Результат | **Одинаковый!** | **Одинаковый!** |

**Важно:** sklearn умнее выбирает метод в зависимости от данных

---
# Ключевые термины

| Термин | Определение |
|--------|-------------|
| **Loss Function** | Функция, измеряющая ошибку модели (MSE) |
| **Gradient** | Вектор частных производных |
| **Learning Rate** | Размер шага при обновлении весов |
| **Iteration** | Один цикл обновления весов |
| **Convergence** | Достижение минимума функции потерь |
| **Epoch** | Полный проход по всем данным |
