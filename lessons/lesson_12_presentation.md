---
marp: true
theme: default
paginate: true
backgroundColor: #fff
---

# Множественная линейная регрессия 

---
## Множественная линейная регрессия

### Формула с одним признаком:
$$y = w_0 + w_1 \cdot x_1$$

### Формула с несколькими признаками:
$$y = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + ... + w_n \cdot x_n$$

Где:
- $y$ - предсказываемое значение
- $w_0$ - intercept (базовое значение)
- $w_1, w_2, ..., w_n$ - коэффициенты для каждого признака
- $x_1, x_2, ..., x_n$ - признаки (features)

---

## Пример: Предсказание зарплаты

**С одним признаком:**
$$Salary = w_0 + w_1 \cdot Experience$$

**С несколькими признаками (Lesson 12):**
$$Salary = w_0 + w_1 \cdot Experience + w_2 \cdot Remote + w_3 \cdot CompanySize$$

### Преимущества:
- Более точные предсказания
- Учитываем больше факторов
- Лучше объясняем вариацию данных (выше R²)

---

## Проблема: Разные масштабы признаков

### Пример данных:

| Признак | Минимум | Максимум | Масштаб |
|---------|---------|----------|---------|
| Experience | 0 | 3 | Маленький |
| Remote Ratio | 0 | 100 | Средний |
| Salary | 15,000 | 450,000 | Большой |

**Проблема:**
- Признаки с большими значениями влияют на модель сильнее
- Градиентный спуск работает медленнее
- Коэффициенты сложно интерпретировать

---

## Feature Scaling
### Два основных метода:

1. **StandardScaler (Стандартизация)**
   $$x_{scaled} = \frac{x - \mu}{\sigma}$$
   - Результат: среднее = 0, std = 1
   - Используется чаще всего

2. **MinMaxScaler (Нормализация)**
   $$x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}$$
   - Результат: значения от 0 до 1
   - Хорошо для данных с границами


---

## Важно: Когда не нужен скейлинг?

### Линейная регрессия технически не требует скейлинга
- Результат будет одинаковым, но коэффициенты будут разными

### Когда скейлинг ОБЯЗАТЕЛЕН:

- **Ridge Regression** - штраф на большие коэффициенты
- **Lasso Regression** - автоматический отбор признаков
- **Support Vector Machines (SVM)** - расстояния критичны
- **K-Nearest Neighbors (KNN)** - вычисляет расстояния
- **Нейронные сети** - градиентный спуск требует равных масштабов
- **Логистическая регрессия** - с регуляризацией обязательно
- **PCA** (анализ главных компонент) - работает с вариациями

---

## Почему скейлинг не влияет на Linear Regression?

### Математическое объяснение:

**До скейлинга:**
$$\hat{y} = w_0 + w_1 \cdot x_1 + w_2 \cdot x_2$$

**После скейлинга** $(x_{scaled} = \frac{x - \mu}{\sigma})$:
$$\hat{y} = w_0' + w_1' \cdot x_{1scaled} + w_2' \cdot x_{2scaled}$$

### Ключевой момент:
- Коэффициенты автоматически адаптируются: $w_1' = w_1 \cdot \sigma_1$
- Предсказания остаются теми же!
- Linear Regression находит оптимальные веса для **любого** масштаба

**Но:** Скейлинг упрощает интерпретацию коэффициентов

---

## Train/Test Split и скейлинг

### КРИТИЧЕСКИ ВАЖНО:

1. Сначала делим на train/test
2. Fit scaler только на TRAIN данных
3. Transform и train, и test данных
```python
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)


scaler.fit(X)  # Используем все данные
X_scaled = scaler.transform(X)
X_train, X_test = train_test_split(X_scaled)
```

Информация из test попадет в train (data leakage)

---

## Контрольные вопросы

1. Что такое множественная линейная регрессия и чем отличается от регрессии с одним признаком?
2. Зачем в модели используют несколько признаков?
3. Почему нельзя делать fit scaler на всех данных сразу?
4. Влияет ли Feature Scaling на предсказания Linear Regression?